{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaTCWvCUWJ-s"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data  import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "fZ_cd7LkWh2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.arange(6, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "jnMsSHYcWoS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "da = DataLoader(t)\n",
        "for item in da:\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsq2NIw6W0TC",
        "outputId": "7da150c7-27f9-4da1-a8e0-3779c24b4c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.])\n",
            "tensor([1.])\n",
            "tensor([2.])\n",
            "tensor([3.])\n",
            "tensor([4.])\n",
            "tensor([5.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = DataLoader(t, batch_size=2, drop_last=False)\n",
        "val = [value.numpy() for value in data]\n",
        "print(val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXf5IDX6XA67",
        "outputId": "5555452f-a592-4df3-bdfd-127299085a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([0., 1.], dtype=float32), array([2., 3.], dtype=float32), array([4., 5.], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "iris = load_iris()"
      ],
      "metadata": {
        "id": "DDErWO9DXQjI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris.target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfHvOdbEiXUV",
        "outputId": "f2cbece4-3ee2-48a0-d709-14aadb399243"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = iris['data']\n",
        "y = iris['target']"
      ],
      "metadata": {
        "id": "FaUlcZv8icds"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=1./3)"
      ],
      "metadata": {
        "id": "Y_AHJWbWin3h"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X_train_norm = (X_train - np.mean(X_train))/np.std(X_train)"
      ],
      "metadata": {
        "id": "IeD4BNIpi7_O"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_norm = torch.from_numpy(X_train_norm).float()"
      ],
      "metadata": {
        "id": "HA8ASipbjh-p"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_norm = torch.from_numpy(y_train)"
      ],
      "metadata": {
        "id": "v-bqQUMTjwHK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset"
      ],
      "metadata": {
        "id": "xzaLfJSOj5zT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TensorDataset(X_train_norm, y_train_norm)"
      ],
      "metadata": {
        "id": "Jh-TbYsRkF7c"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "stbnU_6lkdGx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size =2\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "UQr4PZLdklW4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "    self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = nn.Sigmoid()(x)\n",
        "    x = self.layer2(x)\n",
        "    x = nn.Softmax(dim=1)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "JXCg1EhXlBcj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = X_train_norm.shape[1]\n",
        "hidden_size = 16\n",
        "output_size = 3\n",
        "model = Model(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "LQIA768MncjL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr =learning_rate)"
      ],
      "metadata": {
        "id": "a97vAEWxn8Xy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "log_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "  for x_batch, y_batch in train_dl:\n",
        "    pred = model(x_batch)\n",
        "    loss = loss_fn(pred, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "  if epoch % log_epochs == 0:\n",
        "    print(f'Epoch {epoch} Loss {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDDwVX51obhf",
        "outputId": "c0f75d25-ba5a-4c72-c016-ceb92e4cd78e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss 0.6028\n",
            "Epoch 10 Loss 0.5519\n",
            "Epoch 20 Loss 0.8049\n",
            "Epoch 30 Loss 0.5515\n",
            "Epoch 40 Loss 0.5649\n",
            "Epoch 50 Loss 0.5543\n",
            "Epoch 60 Loss 0.7585\n",
            "Epoch 70 Loss 0.5517\n",
            "Epoch 80 Loss 0.5515\n",
            "Epoch 90 Loss 0.5576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_norm = (X_test - np.mean(X_train))/np.std(X_train)\n",
        "X_test_norm = torch.from_numpy(X_test_norm).float()\n",
        "y_test_norm = torch.from_numpy(y_test)"
      ],
      "metadata": {
        "id": "7eh-U71_qCDJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test = model(X_test_norm)"
      ],
      "metadata": {
        "id": "dyL7y2OFr-UP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = (torch.argmax(pred_test, dim=1) == y_test).float()"
      ],
      "metadata": {
        "id": "cidRqNr5sGig"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = correct.mean()"
      ],
      "metadata": {
        "id": "BLf-KtKFsLNs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Test Acc.: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTxJ15YDsOM3",
        "outputId": "62c63e11-f746-460f-fa85-e7c31af01e32"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Acc.: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'iris_classifier.pt'\n",
        "torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "_a4h1vyEsR2e"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_new = Model(input_size, hidden_size, output_size)\n",
        "model_new.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01v3E7f2taRq",
        "outputId": "93167fbf-48cf-43ac-a47f-999f4b5685a7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-b21dbd0d4824>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_new.load_state_dict(torch.load(path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}